{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tbenst/glia\n",
    "# please install using `python setup.py develop`\n",
    "import glia, os, numpy as np, pandas as pd, av, matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"/home/tyler/Dropbox/temp/180213/\"\n",
    "name = \"R1_E3_AMES_240min_letters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script\n",
    "### read in files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPL program\n",
    "The EyeCandy Programing Language (EPL) is a simple embedded domain-specific programming language that is embedded in JavaScript. EPL programs are used by EyeCandy to generate a sequence of objects that define a declarative rendering for a stimuli.\n",
    "\n",
    "The embedding is shallow and nearly identical to Javascript, and is described in more detail [here](https://github.com/tbenst/eye-candy). The program used for this protocol is below. In brief, we have 10 [ETDRS letters](https://duckduckgo.com/?t=ffab&q=ETDRS+letters&iax=images&ia=images) of one of four sizes presented 100 times each for 0.5 seconds. Each letter stimuli is presented as part of a group of 0.5 seconds of full-screen black (\"SOLID\"), followed by 0.5 seconds of the letter, then 0.5-1 second of black. Groups are organized into cohorts of 10 groups, one for each letter. The order of groups is shuffled, as to reduce bias in acuity from long-timescale changes (e.g. if retinal response degrades over time, later stimuli may be harder to decode).\n",
    "\n",
    "```javascript\n",
    "const metadata = {name: \"letters\", version: \"0.3.0\", inverted: false}\n",
    "let repetitions = 100\n",
    "let sizes = [100,200,250,300]\n",
    "let durations = [0.5]\n",
    "\n",
    "\n",
    "function* measureIntegrity(stimuli,every=5*60) {\n",
    "    // every N seconds, do a flash\n",
    "    let integrityMeta\n",
    "    let elapsedTime = every\n",
    "    for (let s of stimuli) {\n",
    "        if (elapsedTime>=every && s.metadata.block===undefined) {\n",
    "            integrityMeta = {group: r.uuid(), label: \"integrity\"}\n",
    "            yield new Wait(1, integrityMeta)\n",
    "            yield new Solid(0.5, \"white\", integrityMeta)\n",
    "            yield new Wait(2, integrityMeta)\n",
    "            elapsedTime = 0\n",
    "            yield s\n",
    "        } else {\n",
    "            yield s\n",
    "        }\n",
    "        elapsedTime=elapsedTime+s[\"lifespan\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "let letters = [\"C\", \"D\", \"H\", \"K\", \"N\", \"O\", \"R\", \"S\", \"V\", \"Z\"]\n",
    "let x\n",
    "let y\n",
    "let stimuli = []\n",
    "let l\n",
    "let before\n",
    "let after\n",
    "let id\n",
    "let cohort\n",
    "\n",
    "for (let i = 0; i < repetitions; i++) {\n",
    "    for (let size of sizes) {\n",
    "        for (let duration of durations) {\n",
    "            cohort = r.uuid(i)\n",
    "            for (let letter of letters) {\n",
    "                id = r.uuid(i)\n",
    "                // block means \"do not insert a integrity check before me\"\n",
    "                // backgroundColor, letter, x, y, size, color\n",
    "                x = windowWidth/2 - size/2\n",
    "                y = windowHeight/2 + size/2\n",
    "                l = new Letter(duration, \"black\", letter,x,y,size, \n",
    "                    \"white\", {group: id, cohort: cohort, block: true})\n",
    "                before = new Wait(duration, {group: id})\n",
    "                after = new Wait(r.randi(0.5,1), {group: id, block: true})\n",
    "\n",
    "                // before + lit + after = lifespan\n",
    "                // this pads the white flash\n",
    "                stimuli.push([before, l, after])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "r.shuffle(stimuli)\n",
    "\n",
    "let stimulusGenerator = measureIntegrity(flatten(stimuli))\n",
    "for (let s of stimulusGenerator) {\n",
    "    yield s\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_one_file(path):\n",
    "    results = glob(path)\n",
    "    if len(results) == 1:\n",
    "        return results[0]\n",
    "    else:\n",
    "        print(\"found \" + str(len(results)) + \" files instead of one for \" + path)\n",
    "        return results\n",
    "    \n",
    "stimulus_file = glob_one_file(os.path.join(data_directory, name + \".stim\"))\n",
    "frames_file = glob_one_file(os.path.join(data_directory, \"*.log\"))\n",
    "video_file = glob_one_file(os.path.join(data_directory, \"*.mkv\"))\n",
    "\n",
    "# takes ~30 seconds\n",
    "metadata, stimulus_list, method = glia.read_stimulus(stimulus_file)\n",
    "frame_log = pd.read_csv(frames_file)\n",
    "frame_log = frame_log[:-1] # last frame is not encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in retinal responses (may take a minute)\n",
    "retinal_data_file = glob_one_file(data_directory+\"*.npz\")\n",
    "data = np.load(retinal_data_file)\n",
    "shape = np.shape(data['training_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training_data', 'training_target', 'validation_data', 'validation_target']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for uninteresting historical reasons,\n",
    "# this data was split into 60% training, 40% validation 0% test.\n",
    "# Feel free to concatenate & re-split into a more standard train/test\n",
    "# split\n",
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 660, 500, 8, 8, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_sizes x num_examples x time(ms) x H x W x C\n",
    "# note that channel here means a spike-sorted unit (purported neuron)\n",
    "# associated with a particular electrode\n",
    "# Many channels are all zero, as no unit was found at that channel\n",
    "# each number is the number of spikes (Action Potentials) in that neuron per ms\n",
    "# theoretically, with perfect spike sorting, this cannot occur more than once\n",
    "# every 2-3 ms, but we encode as int8 in case two units are assigned to one channel\n",
    "data['training_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 660)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_sizes x num_examples\n",
    "data['training_target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['BLANK', 'C', 'D', 'H', 'K', 'N', 'O', 'R', 'S', 'V', 'Z']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.unique(data['training_target']))\n",
    "glia.letter_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start_time': 8.49776,\n",
       "  'stimulus': {'age': 0,\n",
       "   'backgroundColor': 'black',\n",
       "   'lifespan': 0.5,\n",
       "   'metadata': {'group': '077b07ed-423d-42dc-a252-ac0fb2acb823'},\n",
       "   'stimulusIndex': 3,\n",
       "   'stimulusType': 'WAIT'}},\n",
       " {'start_time': 9.02996,\n",
       "  'stimulus': {'age': 0,\n",
       "   'backgroundColor': 'black',\n",
       "   'color': 'white',\n",
       "   'letter': 'K',\n",
       "   'lifespan': 0.5,\n",
       "   'metadata': {'block': True,\n",
       "    'cohort': '510a24ca-eed7-4c65-b38c-6f8c7b409f2a',\n",
       "    'group': '077b07ed-423d-42dc-a252-ac0fb2acb823'},\n",
       "   'size': 250,\n",
       "   'stimulusIndex': 4,\n",
       "   'stimulusType': 'LETTER',\n",
       "   'x': 515,\n",
       "   'y': 525}},\n",
       " {'start_time': 9.57948,\n",
       "  'stimulus': {'age': 0,\n",
       "   'backgroundColor': 'black',\n",
       "   'lifespan': 0.5,\n",
       "   'metadata': {'block': True,\n",
       "    'group': '077b07ed-423d-42dc-a252-ac0fb2acb823'},\n",
       "   'stimulusIndex': 5,\n",
       "   'stimulusType': 'WAIT'}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at three stimuli that includes a letter\n",
    "stimulus_list[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>framenum</th>\n",
       "      <th>time</th>\n",
       "      <th>stimulusIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10573.710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10607.083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10640.455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10657.141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10673.826</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   framenum       time  stimulusIndex\n",
       "0         0  10573.710              0\n",
       "1         1  10607.083              0\n",
       "2         2  10640.455              0\n",
       "3         3  10657.141              0\n",
       "4         4  10673.826              0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WARNING: times (in ms) are incorrect for the frame log--we did not originally save\n",
    "# the video for this recording, so I repeated the program saving the frames\n",
    "# as to get pixel values. The stimulusIndex should be correct, however.\n",
    "frame_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract one frame for each letter stimuli\n",
    "This is rather boring data munging--feel free to skip reading the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes we drop a frame on the first render of a stimuli, so take the last frame to be conservative\n",
    "# this index in frame-space\n",
    "last_frame_idx_of_stimuli = np.concatenate([\n",
    "    np.where(np.diff(frame_log.stimulusIndex))[0],\n",
    "    [frame_log.stimulusIndex.iloc[-1]]\n",
    "])\n",
    "\n",
    "assert len(stimulus_list) == len(last_frame_idx_of_stimuli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_stim = list(filter(\n",
    "    lambda s: s['stimulus']['stimulusType']=='LETTER',\n",
    "    stimulus_list))\n",
    "letter_stim_idx = [s['stimulus']['stimulusIndex']\n",
    "                         for s in letter_stim]\n",
    "\n",
    "# # take last frame of stimulus as letter is static\n",
    "letter_stim_frame_idx = last_frame_idx_of_stimuli[letter_stim_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first frame from video so we know the size\n",
    "container = av.open(video_file)\n",
    "for frame in container.decode(video=0):\n",
    "    first_frame = frame.to_ndarray(format='bgr24')\n",
    "    break\n",
    "\n",
    "H, W, C = first_frame.shape\n",
    "dtype = first_frame.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of letters: 10\n",
      "Number of sizes: 4\n"
     ]
    }
   ],
   "source": [
    "# extract letters & sizes from the stimulus list\n",
    "\n",
    "letters = sorted(list(set(map(\n",
    "    lambda s: s['stimulus']['letter'],\n",
    "    letter_stim))))\n",
    "n_letters = len(letters)\n",
    "\n",
    "sizes = sorted(set(map(\n",
    "    lambda s: s['stimulus']['size'],\n",
    "    letter_stim)))\n",
    "n_sizes = len(sizes)\n",
    "\n",
    "print(f\"Number of letters: {n_letters}\")\n",
    "print(f\"Number of sizes: {n_sizes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab only one frame of the 100 repetitions\n",
    "# TODO: include BLANK and use \n",
    "# 10 letters plus 1 for BLANK (all black)\n",
    "size_letter_to_frame = np.zeros([n_sizes, n_letters+1, H, W, C], dtype=dtype)\n",
    "\n",
    "size_letter_set = set()\n",
    "\n",
    "# map a frame index to (size, letter) tuple\n",
    "# we use only the first frame index to limit\n",
    "# amount of video we must loop through\n",
    "idx_to_size_letter = {}\n",
    "\n",
    "size_to_idx = {s: i for i,s in enumerate(sizes)}\n",
    "\n",
    "for s in letter_stim_idx:\n",
    "    # note that s is an index in stimulus-space\n",
    "    stim = stimulus_list[s]\n",
    "    size = stim[\"stimulus\"][\"size\"]\n",
    "    letter = stim[\"stimulus\"][\"letter\"]\n",
    "    key = (size, letter)\n",
    "    if not key in size_letter_set:\n",
    "        # add to set so lines below occur on earliest frame idx only \n",
    "        # for this (size, letter)\n",
    "        size_letter_set.add(key)\n",
    "        # we index by the stimulus idx\n",
    "        idx = last_frame_idx_of_stimuli[s]\n",
    "        idx_to_size_letter[idx] = (size_to_idx[size], glia.letter_map[letter])\n",
    "        \n",
    "# ensure all size x letter combinations are present\n",
    "assert len(size_letter_set)==n_sizes*n_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5008accb0eb417286393a65c7c4d518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16592), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame_indices = sorted(list(idx_to_size_letter.keys()))\n",
    "next_frame_idx = frame_indices.pop(0)\n",
    "\n",
    "# iterate through frames, and match to each entry in frame_indices\n",
    "# seeking by time would be faster, but does not map 1:1 with frame index\n",
    "# should take < 1 minute\n",
    "for n,frame in tqdm(enumerate(container.decode(video=0)),total=frame_indices[-1]):\n",
    "    if n==(next_frame_idx):\n",
    "        size, letter = idx_to_size_letter[next_frame_idx]\n",
    "        size_letter_to_frame[size, letter] = frame.to_ndarray(format='bgr24')\n",
    "        if len(frame_indices)!=0:\n",
    "            next_frame_idx = frame_indices.pop(0)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_for_letter(size:float, letter:str, size_letter_to_frame=size_letter_to_frame,\n",
    "                         size_to_idx=size_to_idx):\n",
    "    \"Given the size and the letter, return a H x W x C ndarray.\"\n",
    "    s = size_to_idx[size]\n",
    "    l = glia.letter_classes[letter]\n",
    "    return size_letter_to_frame[s,l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the size_letter_to_frame ndarray for first 3 classes\n",
    "\n",
    "fig, axes = plt.subplots(3,4, figsize=(10,6))\n",
    "for s, size in enumerate(sizes):\n",
    "    for l, letter in enumerate(glia.letter_classes[:3]):\n",
    "        ax = axes[l,s]\n",
    "        ax.imshow(size_letter_to_frame[s,l])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f\"class: {l}, size: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== SVC for size 100 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/.conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        40\n",
      "           1       0.35      0.50      0.41        40\n",
      "           2       0.62      0.40      0.48        40\n",
      "           3       0.38      0.30      0.33        40\n",
      "           4       0.65      0.80      0.72        40\n",
      "           5       0.54      0.55      0.54        40\n",
      "           6       0.26      0.40      0.32        40\n",
      "           7       0.54      0.38      0.44        40\n",
      "           8       0.52      0.38      0.43        40\n",
      "           9       0.78      0.78      0.78        40\n",
      "          10       0.63      0.60      0.62        40\n",
      "\n",
      "    accuracy                           0.55       440\n",
      "   macro avg       0.57      0.55      0.55       440\n",
      "weighted avg       0.57      0.55      0.55       440\n",
      "\n",
      "\n",
      "       BLANK   C   D   H   K   N   O   R   S   V   Z\n",
      "BLANK     39   1   0   0   0   0   0   0   0   0   0\n",
      "C          0  20   1   0   0   0  12   1   1   0   5\n",
      "D          0   7  16   1   2   2   3   2   2   0   5\n",
      "H          0   1   0  12   9  11   3   2   0   1   1\n",
      "K          0   3   0   1  32   1   0   2   1   0   0\n",
      "N          0   0   3   9   4  22   0   0   1   1   0\n",
      "O          0  14   0   0   0   2  16   2   3   2   1\n",
      "R          0   3   2   8   2   3   3  15   2   1   1\n",
      "S          0   2   1   0   0   0  14   3  15   4   1\n",
      "V          0   1   1   1   0   0   6   0   0  31   0\n",
      "Z          0   5   2   0   0   0   4   1   4   0  24\n",
      "==== SVC for size 200 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/.conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       0.51      0.53      0.52        40\n",
      "           2       0.97      0.70      0.81        40\n",
      "           3       0.90      0.93      0.91        40\n",
      "           4       0.80      0.82      0.81        40\n",
      "           5       0.90      0.93      0.91        40\n",
      "           6       0.45      0.47      0.46        40\n",
      "           7       0.86      0.78      0.82        40\n",
      "           8       0.71      0.88      0.79        40\n",
      "           9       0.97      0.95      0.96        40\n",
      "          10       0.95      0.97      0.96        40\n",
      "\n",
      "    accuracy                           0.81       440\n",
      "   macro avg       0.82      0.81      0.81       440\n",
      "weighted avg       0.82      0.81      0.81       440\n",
      "\n",
      "\n",
      "       BLANK   C   D   H   K   N   O   R   S   V   Z\n",
      "BLANK     40   0   0   0   0   0   0   0   0   0   0\n",
      "C          0  21   1   0   0   0  17   0   1   0   0\n",
      "D          0   1  28   0   1   0   6   0   4   0   0\n",
      "H          0   0   0  37   0   2   0   1   0   0   0\n",
      "K          0   0   0   0  33   0   0   2   3   0   2\n",
      "N          0   0   0   2   0  37   0   0   0   1   0\n",
      "O          0  18   0   0   0   0  19   0   3   0   0\n",
      "R          0   0   0   2   5   0   0  31   2   0   0\n",
      "S          0   1   0   0   2   0   0   2  35   0   0\n",
      "V          0   0   0   0   0   2   0   0   0  38   0\n",
      "Z          0   0   0   0   0   0   0   0   1   0  39\n",
      "==== SVC for size 250 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/.conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       0.69      0.95      0.80        40\n",
      "           2       1.00      0.97      0.99        40\n",
      "           3       0.98      1.00      0.99        40\n",
      "           4       0.98      1.00      0.99        40\n",
      "           5       0.95      0.88      0.91        40\n",
      "           6       0.96      0.57      0.72        40\n",
      "           7       0.90      0.95      0.93        40\n",
      "           8       0.93      0.97      0.95        40\n",
      "           9       1.00      0.97      0.99        40\n",
      "          10       0.97      0.97      0.97        40\n",
      "\n",
      "    accuracy                           0.93       440\n",
      "   macro avg       0.94      0.93      0.93       440\n",
      "weighted avg       0.94      0.93      0.93       440\n",
      "\n",
      "\n",
      "       BLANK   C   D   H   K   N   O   R   S   V   Z\n",
      "BLANK     40   0   0   0   0   0   0   0   0   0   0\n",
      "C          0  38   0   0   0   0   1   0   1   0   0\n",
      "D          0   0  39   0   0   0   0   1   0   0   0\n",
      "H          0   0   0  40   0   0   0   0   0   0   0\n",
      "K          0   0   0   0  40   0   0   0   0   0   0\n",
      "N          0   0   0   1   1  35   0   3   0   0   0\n",
      "O          0  17   0   0   0   0  23   0   0   0   0\n",
      "R          0   0   0   0   0   2   0  38   0   0   0\n",
      "S          0   0   0   0   0   0   0   0  39   0   1\n",
      "V          0   0   0   0   0   0   0   0   1  39   0\n",
      "Z          0   0   0   0   0   0   0   0   1   0  39\n",
      "==== SVC for size 300 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/.conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       0.95      0.93      0.94        40\n",
      "           2       1.00      1.00      1.00        40\n",
      "           3       0.98      1.00      0.99        40\n",
      "           4       1.00      1.00      1.00        40\n",
      "           5       1.00      0.97      0.99        40\n",
      "           6       0.93      0.95      0.94        40\n",
      "           7       1.00      1.00      1.00        40\n",
      "           8       1.00      1.00      1.00        40\n",
      "           9       1.00      1.00      1.00        40\n",
      "          10       1.00      1.00      1.00        40\n",
      "\n",
      "    accuracy                           0.99       440\n",
      "   macro avg       0.99      0.99      0.99       440\n",
      "weighted avg       0.99      0.99      0.99       440\n",
      "\n",
      "\n",
      "       BLANK   C   D   H   K   N   O   R   S   V   Z\n",
      "BLANK     40   0   0   0   0   0   0   0   0   0   0\n",
      "C          0  37   0   0   0   0   3   0   0   0   0\n",
      "D          0   0  40   0   0   0   0   0   0   0   0\n",
      "H          0   0   0  40   0   0   0   0   0   0   0\n",
      "K          0   0   0   0  40   0   0   0   0   0   0\n",
      "N          0   0   0   1   0  39   0   0   0   0   0\n",
      "O          0   2   0   0   0   0  38   0   0   0   0\n",
      "R          0   0   0   0   0   0   0  40   0   0   0\n",
      "S          0   0   0   0   0   0   0   0  40   0   0\n",
      "V          0   0   0   0   0   0   0   0   0  40   0\n",
      "Z          0   0   0   0   0   0   0   0   0   0  40\n"
     ]
    }
   ],
   "source": [
    "# this may take a few minutes\n",
    "training_100ms = glia.bin_100ms(np.expand_dims(data[\"training_data\"],0))\n",
    "validation_100ms = glia.bin_100ms(np.expand_dims(data[\"validation_data\"],0))\n",
    "for i, size in enumerate(sizes):\n",
    "    print(f'==== SVC for size {size} ====')\n",
    "    # note: no expand dims, hardcoded 1 ncondition\n",
    "    training_target = data[\"training_target\"][i]\n",
    "    validation_target = data[\"validation_target\"][i]\n",
    "    svr = svm.SVC()\n",
    "    parameters = {'C': [1, 10, 100, 1000],\n",
    "                  'gamma': [0.001, 0.0001]},\n",
    "    clf = GridSearchCV(svr, parameters, n_jobs=12)\n",
    "    report, confusion = glia.classifier_helper(clf,\n",
    "        (training_100ms[0,i], training_target),\n",
    "        (validation_100ms[0,i], validation_target))\n",
    "    print(report+'\\n')\n",
    "    print(confusion)\n",
    "    \n",
    "# near perfect precision & recall at size=300 declines to only ~55%\n",
    "# at size=100. Note that chance is 9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel value inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, cv2\n",
    "from torch import nn, optim, utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F\n",
    "import torchvision.utils\n",
    "from dataclasses import dataclass\n",
    "\n",
    "torch.manual_seed(20200525); # reproducible analysis\n",
    "np.random.seed(seed=20200525)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9d38343668>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# choose crop parameters\n",
    "biggest = max(sizes)\n",
    "w1 = int((W - biggest*1.5) / 2)\n",
    "w2 = W - w1\n",
    "h1 = int((H - biggest*1.5) / 2)\n",
    "h2 = H - h1\n",
    "plt.imshow(size_letter_to_frame[-1,3,h1:h2,w1:w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 660, 3200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_100ms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(N, T, H, W, C), (N, 2)\n",
      "train shapes:  (3960, 3200) (3960, 2)\n",
      "validation shapes:  (220, 3200) (220, 2)\n",
      "test shapes:  (220, 3200) (220, 2)\n"
     ]
    }
   ],
   "source": [
    "all_data = np.concatenate([training_100ms, validation_100ms], axis=2)[0]\n",
    "all_targets = np.concatenate([data[\"training_target\"], data[\"validation_target\"]], axis=1)\n",
    "\n",
    "# reshape to remove size dimension\n",
    "n_examples = np.product(all_data.shape[:2])\n",
    "X = np.zeros([n_examples, *all_data.shape[2:]])\n",
    "# to avoid blowing up RAM, we store (size, letter)\n",
    "# and will use dataloader to get the frame during training\n",
    "Y = np.zeros([n_examples, 2], dtype=np.uint8)\n",
    "for s,size in enumerate(sizes):\n",
    "    for n,(x,l) in enumerate(zip(all_data[s], all_targets[s])):\n",
    "        i = n * n_sizes + s\n",
    "        X[i] = x # time x H x W x C\n",
    "        Y[i,0] = s # size index\n",
    "        Y[i,1] = l # letter index\n",
    "        \n",
    "# del all_data, all_targets\n",
    "\n",
    "# 90% train, 5% validation, 5% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=1)\n",
    "\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=1)\n",
    "\n",
    "# del X, Y\n",
    "\n",
    "print(\"(N, T, H, W, C), (N, 2)\")\n",
    "print(\"train shapes: \", X_train.shape, Y_train.shape)\n",
    "print(\"validation shapes: \", X_val.shape, Y_val.shape)\n",
    "print(\"test shapes: \", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 11, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class LetterData(Dataset):\n",
    "    X: np.ndarray\n",
    "    Y: np.ndarray\n",
    "    size_letter_to_frame: np.ndarray\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        size_idx = self.Y[i,0]\n",
    "        letter_idx = self.Y[i,1]\n",
    "        frame = self.size_letter_to_frame[size_idx, letter_idx]\n",
    "        return self.X[i], frame\n",
    "        \n",
    "def resize_4d(images, fx, fy, interpolation=cv2.INTER_LINEAR, out=\"ndarray\",\n",
    "        is_tiff=False):\n",
    "    im = cv2.resize((images[0,0]), None, fx=fx, fy=fy, interpolation=interpolation)\n",
    "    if out==\"ndarray\":\n",
    "        new = np.zeros([images.shape[0],images.shape[1], *im.shape],\n",
    "            dtype = np.float32)\n",
    "    elif out==\"memmap\":\n",
    "        new = np.memmap(\"glia_temp_memmap.mmap\", np.float32, \"w+\",\n",
    "            (images.shape[0],images.shape[1], im.shape[0],im.shape[1]))\n",
    "    for b, vol in enumerate(images):\n",
    "        for z, img in enumerate(vol):\n",
    "            new[b,z] = cv2.resize((img), None, fx=fx, fy=fy,\n",
    "                interpolation=interpolation)\n",
    "    return new\n",
    "\n",
    "# downsample to 0.5x\n",
    "small_size_letter_to_frame = resize_4d(size_letter_to_frame[:,:,h1:h2, w1:w2],1/16,1/16)[...,0]\n",
    "# binarize\n",
    "small_size_letter_to_frame = (small_size_letter_to_frame > 128).astype(np.float32)\n",
    "\n",
    "train_data = LetterData(X_train, Y_train, small_size_letter_to_frame)\n",
    "val_data = LetterData(X_val, Y_val, small_size_letter_to_frame)\n",
    "test_data = LetterData(X_test, Y_test, small_size_letter_to_frame)\n",
    "print(small_size_letter_to_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at smallest letter with downsampling\n",
    "plt.imshow(small_size_letter_to_frame[0,1])\n",
    "small_H, small_W = small_size_letter_to_frame[0,1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether or not to use cuda\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kingma & Welling (2014) style variational autoencoder\n",
    "\n",
    "# subclass PyTorch Module for reverse-mode autodifferentiation \n",
    "# for easy backpropogation of loss gradient\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_latent, n_hidden2, n_output,\n",
    "                 nonlinearity=F.sigmoid):\n",
    "        super(VAE, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.n_hidden = n_hidden\n",
    "        self.nonlinearity = F.sigmoid\n",
    "                \n",
    "        # Encoder layers\n",
    "        self.hidden_encoder = nn.Linear(n_input, n_hidden)\n",
    "        # mean encoding layer \n",
    "        self.mean_encoder = nn.Linear(n_hidden, n_latent)\n",
    "        # log variance encoding layer \n",
    "        self.logvar_encoder = nn.Linear(n_hidden, n_latent)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.hidden_decoder = nn.Linear(n_latent, n_hidden2)\n",
    "        self.reconstruction_decoder = nn.Linear(n_hidden2, n_output)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.nonlinearity(self.hidden_encoder(x))\n",
    "        return self.mean_encoder(h1), self.logvar_encoder(h1)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"Reparameterize out stochastic node so the gradient can propogate \n",
    "           deterministically.\"\"\"\n",
    "\n",
    "        if self.training:\n",
    "            standard_deviation = torch.exp(0.5*logvar)\n",
    "            # sample from unit gaussian with same shape as standard_deviation\n",
    "            epsilon = torch.randn_like(standard_deviation)\n",
    "            return epsilon * standard_deviation + mean\n",
    "        else:\n",
    "            return mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.nonlinearity(self.hidden_decoder(z))\n",
    "        # bound output to (0,1)\n",
    "        return F.sigmoid(self.reconstruction_decoder(h3))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"A special method in PyTorch modules that is called by __call__\"\n",
    "        mean, logvar = self.encode(x)\n",
    "        # sample an embedding, z\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        # return the (sampled) reconstruction, mean, and log variance\n",
    "        return self.decode(z), mean, logvar\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(recon_y, y, mu, logvar, beta=1):\n",
    "    \"Reconstruction + KL divergence losses summed over all elements and batch.\"\n",
    "    BCE = F.binary_cross_entropy(recon_y, y, size_average=False)\n",
    "\n",
    "    # we want KLD = - 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # where sigma is standard deviation and mu is mean\n",
    "    # (see Appendix B of https://arxiv.org/abs/1312.6114)\n",
    "    \n",
    "    # see https://openreview.net/forum?id=Sy2fzU9gl for info on choosing Beta\n",
    "    \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "\n",
    "    return BCE + beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, train_loader, beta=1., log_interval=10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        bz = data[0].shape[0]\n",
    "        # flatten batch x height x width x channel into batch x nFeatures\n",
    "        X = data[0].reshape(bz, -1).to(device)\n",
    "        Y = data[1].reshape(bz, -1).to(device) # flatten\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred, mu, logvar = model(X)\n",
    "        loss = loss_function(Y_pred, Y, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch: {} Average loss: {:.4f}'.format(\n",
    "              epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch, model, test_loader, H, W, beta=1., log_interval=10,\n",
    "         save_image=True, folder=\"results\", name=\"Test\"):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            bz = data[0].shape[0]\n",
    "            \n",
    "            # flatten batch x height x width x channel into batch x nFeatures\n",
    "            X = data[0].reshape(bz, -1).to(device)\n",
    "            Y = data[1].reshape(bz, -1).to(device) # flatten\n",
    "\n",
    "            Y_pred, mu, logvar = model(X)\n",
    "            test_loss += loss_function(Y_pred, Y, mu, logvar).item()\n",
    "            if (i == 0) and save_image:\n",
    "                n = min(X.size(0), 15)\n",
    "                comparison = torch.cat([Y[:n].view(-1,1,H,W),\n",
    "                                   Y_pred[:n].view(-1, 1, H, W)])\n",
    "                torchvision.utils.save_image(comparison.cpu(),\n",
    "                         folder+f'/{name}_reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if epoch % log_interval == 0:\n",
    "        print(f'====> {name} ' + 'set loss: {:.4f}'.format(test_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800800\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(LetterData(X_train.astype(np.float32),\n",
    "        Y_train, small_size_letter_to_frame),\n",
    "    batch_size=64, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(LetterData(X_val.astype(np.float32),\n",
    "        Y_val, small_size_letter_to_frame),\n",
    "    batch_size=64, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(LetterData(X_test.astype(np.float32),\n",
    "        Y_test, small_size_letter_to_frame),\n",
    "    batch_size=64, shuffle=True, pin_memory=True)\n",
    "\n",
    "nfeatures = 28**2\n",
    "# we use a latent space of dimension 2 as to get an easy-to-visualize manifold\n",
    "# (see mnist/sample_*.png while running next cell)\n",
    "n_input = np.prod(X_train.shape[1:])\n",
    "n_hidden = 200\n",
    "n_latent = 10\n",
    "n_hidden2 = 200\n",
    "beta = 0.9\n",
    "nonlinearity = F.tanh\n",
    "n_output = np.product(small_size_letter_to_frame.shape[2:])\n",
    "est_params = n_input*n_hidden + n_hidden*n_latent + n_latent*n_hidden2 + n_hidden2*n_output\n",
    "print(est_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/.conda/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/tyler/.conda/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 Average loss: 80.0460\n",
      "====> Validation set loss: 75.9959\n",
      "Epoch: 20 Average loss: 54.2076\n",
      "====> Validation set loss: 52.2593\n",
      "Epoch: 30 Average loss: 36.7934\n",
      "====> Validation set loss: 37.7081\n",
      "Epoch: 40 Average loss: 28.1926\n",
      "====> Validation set loss: 30.9886\n",
      "Epoch: 50 Average loss: 23.3606\n",
      "====> Validation set loss: 26.8491\n",
      "Epoch: 60 Average loss: 20.1948\n",
      "====> Validation set loss: 24.6021\n",
      "Epoch: 70 Average loss: 17.9687\n",
      "====> Validation set loss: 23.1866\n",
      "Epoch: 80 Average loss: 16.6226\n",
      "====> Validation set loss: 22.7005\n",
      "Epoch: 90 Average loss: 15.3188\n",
      "====> Validation set loss: 21.8530\n",
      "Epoch: 100 Average loss: 14.5636\n",
      "====> Validation set loss: 21.1226\n"
     ]
    }
   ],
   "source": [
    "model = VAE(n_input, n_hidden, n_latent, n_hidden2, n_output,\n",
    "            nonlinearity=nonlinearity).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=9e-4)\n",
    "!mkdir -p letters_viz && rm letters_viz/*\n",
    "\n",
    "# this will take two minutes to run or so\n",
    "# As it does, check out the letters_viz folder!\n",
    "# each epoch, reconstruction examples are saved (original on top) \n",
    "# select files on right, then refresh, and double click image\n",
    "# click bottom right corner of image to resize\n",
    "\n",
    "nepochs = 100\n",
    "\n",
    "# make grid of z1 x z2 where z1,z2 \\elem (-3.5,-2.5, ..., 3.5)\n",
    "nrow = 25\n",
    "latents = torch.zeros(nrow,nrow,n_latent)\n",
    "z1_tick = np.linspace(-3.5,3.5,nrow)\n",
    "z2_tick = np.linspace(-3.5,3.5,nrow)\n",
    "for i, z1 in enumerate(z1_tick):\n",
    "    for j, z2 in enumerate(z2_tick):\n",
    "        latents[i,j,[0,1]] = torch.tensor([z1,z2])\n",
    "latents = latents.to(device)\n",
    "\n",
    "for epoch in range(1, nepochs + 1):\n",
    "    if epoch % 10 == 0:\n",
    "        save_image = True\n",
    "    else:\n",
    "        save_image = False\n",
    "    train(epoch, model, optimizer, train_loader, beta=beta)\n",
    "    test(epoch, model, val_loader, small_H, small_W, beta=beta,\n",
    "         save_image=save_image, folder='letters_viz', name=\"Validation\")\n",
    "    if save_image:\n",
    "        with torch.no_grad():\n",
    "            latent_space = model.decode(latents.view(-1,n_latent)).cpu()\n",
    "            torchvision.utils.save_image(latent_space.view(-1, 1, small_H, small_W),\n",
    "                       'letters_viz/sample_' + str(epoch) + '.png',nrow=nrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 23.3024\n"
     ]
    }
   ],
   "source": [
    "test(epoch, model, test_loader, small_H, small_W, beta=beta,\n",
    "         save_image=save_image, folder='letters_viz', name=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
