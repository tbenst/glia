{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tbenst/glia\n",
    "# please install using `python setup.py develop`\n",
    "import glia, os, numpy as np, pandas as pd, av, matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"/home/tyler/Dropbox/data/uw/200505/\"\n",
    "name = \"R2_E3_BENAQ_490min_letters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glob_one_file(path):\n",
    "    results = glob(path)\n",
    "    if len(results) == 1:\n",
    "        return results[0]\n",
    "    else:\n",
    "        print(\"found \" + str(len(results)) + \" files instead of one for \" + path)\n",
    "        return results\n",
    "    \n",
    "stimulus_file = glob_one_file(os.path.join(data_directory, name + \".stim\"))\n",
    "frames_file = glob_one_file(os.path.join(data_directory, \"*.log\"))\n",
    "video_file = glob_one_file(os.path.join(data_directory, \"*.mkv\"))\n",
    "\n",
    "# takes ~30 seconds\n",
    "metadata, stimulus_list, method = glia.read_stimulus(stimulus_file)\n",
    "frame_log = pd.read_csv(frames_file)\n",
    "frame_log = frame_log[:-1] # last frame is not encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in retinal responses (may take a minute)\n",
    "retinal_data_file = glob_one_file(data_directory+\"*.npz\")\n",
    "data = np.load(retinal_data_file)\n",
    "shape = np.shape(data['training_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training_data', 'training_target', 'validation_data', 'validation_target']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for uninteresting historical reasons,\n",
    "# this data was split into 60% training, 40% validation 0% test.\n",
    "# Feel free to concatenate & re-split into a more standard train/test\n",
    "# split\n",
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 165, 500, 8, 8, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_sizes x num_examples x time(ms) x H x W x C\n",
    "# note that channel here means a spike-sorted unit (purported neuron)\n",
    "# associated with a particular electrode\n",
    "# Many channels are all zero, as no unit was found at that channel\n",
    "# each number is the number of spikes (Action Potentials) in that neuron per ms\n",
    "# theoretically, with perfect spike sorting, this cannot occur more than once\n",
    "# every 2-3 ms, but we encode as int8 in case two units are assigned to one channel\n",
    "data['training_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 165)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_sizes x num_examples\n",
    "data['training_target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['BLANK', 'C', 'D', 'H', 'K', 'N', 'O', 'R', 'S', 'V', 'Z']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.unique(data['training_target']))\n",
    "glia.letter_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start_time': 7.27236,\n",
       "  'stimulus': {'age': 0,\n",
       "   'backgroundColor': 'black',\n",
       "   'lifespan': 0.5,\n",
       "   'metadata': {'group': '468a605b-4fe2-446a-a6cb-bb9ea5480db8'},\n",
       "   'stimulusIndex': 3,\n",
       "   'stimulusType': 'WAIT'}},\n",
       " {'start_time': 7.78428,\n",
       "  'stimulus': {'age': 0,\n",
       "   'backgroundColor': 'black',\n",
       "   'color': 'white',\n",
       "   'letter': 'S',\n",
       "   'lifespan': 0.5,\n",
       "   'metadata': {'block': True,\n",
       "    'cohort': '693cd738-73e0-454d-b920-9605eeee781c',\n",
       "    'group': '468a605b-4fe2-446a-a6cb-bb9ea5480db8'},\n",
       "   'size': 300,\n",
       "   'stimulusIndex': 4,\n",
       "   'stimulusType': 'LETTER',\n",
       "   'x': 490,\n",
       "   'y': 550}},\n",
       " {'start_time': 8.32268,\n",
       "  'stimulus': {'age': 0,\n",
       "   'backgroundColor': 'black',\n",
       "   'lifespan': 0.6333333333333333,\n",
       "   'metadata': {'block': True,\n",
       "    'group': '468a605b-4fe2-446a-a6cb-bb9ea5480db8'},\n",
       "   'stimulusIndex': 5,\n",
       "   'stimulusType': 'WAIT'}}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at three stimuli that includes a letter\n",
    "stimulus_list[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>framenum</th>\n",
       "      <th>time</th>\n",
       "      <th>stimulusIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12720.003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12736.723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12753.442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12770.163</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12786.884</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   framenum       time  stimulusIndex\n",
       "0         0  12720.003              0\n",
       "1         1  12736.723              0\n",
       "2         2  12753.442              0\n",
       "3         3  12770.163              0\n",
       "4         4  12786.884              0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WARNING: times (in ms) are incorrect for the frame log--we did not originally save\n",
    "# the video for this recording, so I repeated the program saving the frames\n",
    "# as to get pixel values. The stimulusIndex should be correct, however.\n",
    "frame_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract one frame for each letter stimuli\n",
    "This is rather boring data munging--feel free to skip reading the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes we drop a frame on the first render of a stimuli, so take the last frame to be conservative\n",
    "# this index in frame-space\n",
    "last_frame_idx_of_stimuli = np.concatenate([\n",
    "    np.where(np.diff(frame_log.stimulusIndex))[0],\n",
    "    [frame_log.stimulusIndex.iloc[-1]]\n",
    "])\n",
    "\n",
    "assert len(stimulus_list) == len(last_frame_idx_of_stimuli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_stim = list(filter(\n",
    "    lambda s: s['stimulus']['stimulusType']=='LETTER',\n",
    "    stimulus_list))\n",
    "letter_stim_idx = [s['stimulus']['stimulusIndex']\n",
    "                         for s in letter_stim]\n",
    "\n",
    "# # take last frame of stimulus as letter is static\n",
    "letter_stim_frame_idx = last_frame_idx_of_stimuli[letter_stim_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first frame from video so we know the size\n",
    "container = av.open(video_file)\n",
    "for frame in container.decode(video=0):\n",
    "    first_frame = frame.to_ndarray(format='bgr24')\n",
    "    break\n",
    "\n",
    "H, W, C = first_frame.shape\n",
    "dtype = first_frame.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of letters: 10\n",
      "Number of sizes: 7\n"
     ]
    }
   ],
   "source": [
    "# extract letters & sizes from the stimulus list\n",
    "\n",
    "letters = sorted(list(set(map(\n",
    "    lambda s: s['stimulus']['letter'],\n",
    "    letter_stim))))\n",
    "n_letters = len(letters)\n",
    "\n",
    "sizes = sorted(set(map(\n",
    "    lambda s: s['stimulus']['size'],\n",
    "    letter_stim)))\n",
    "n_sizes = len(sizes)\n",
    "\n",
    "print(f\"Number of letters: {n_letters}\")\n",
    "print(f\"Number of sizes: {n_sizes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab only one frame of the 100 repetitions\n",
    "# TODO: include BLANK and use \n",
    "# 10 letters plus 1 for BLANK (all black)\n",
    "size_letter_to_frame = np.zeros([n_sizes, n_letters+1, H, W, C], dtype=dtype)\n",
    "\n",
    "size_letter_set = set()\n",
    "\n",
    "# map a frame index to (size, letter) tuple\n",
    "# we use only the first frame index to limit\n",
    "# amount of video we must loop through\n",
    "idx_to_size_letter = {}\n",
    "\n",
    "size_to_idx = {s: i for i,s in enumerate(sizes)}\n",
    "\n",
    "for s in letter_stim_idx:\n",
    "    # note that s is an index in stimulus-space\n",
    "    stim = stimulus_list[s]\n",
    "    size = stim[\"stimulus\"][\"size\"]\n",
    "    letter = stim[\"stimulus\"][\"letter\"]\n",
    "    key = (size, letter)\n",
    "    if not key in size_letter_set:\n",
    "        # add to set so lines below occur on earliest frame idx only \n",
    "        # for this (size, letter)\n",
    "        size_letter_set.add(key)\n",
    "        # we index by the stimulus idx\n",
    "        idx = last_frame_idx_of_stimuli[s]\n",
    "        idx_to_size_letter[idx] = (size_to_idx[size], glia.letter_map[letter])\n",
    "        \n",
    "# ensure all size x letter combinations are present\n",
    "assert len(size_letter_set)==n_sizes*n_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d648a7f659bf4fb8b78c979257f24ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38997), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame_indices = sorted(list(idx_to_size_letter.keys()))\n",
    "next_frame_idx = frame_indices.pop(0)\n",
    "\n",
    "# iterate through frames, and match to each entry in frame_indices\n",
    "# seeking by time would be faster, but does not map 1:1 with frame index\n",
    "# should take < 1 minute\n",
    "for n,frame in tqdm(enumerate(container.decode(video=0)),total=frame_indices[-1]):\n",
    "    if n==(next_frame_idx):\n",
    "        size, letter = idx_to_size_letter[next_frame_idx]\n",
    "        size_letter_to_frame[size, letter] = frame.to_ndarray(format='bgr24')\n",
    "        if len(frame_indices)!=0:\n",
    "            next_frame_idx = frame_indices.pop(0)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_for_letter(size:float, letter:str, size_letter_to_frame=size_letter_to_frame,\n",
    "                         size_to_idx=size_to_idx):\n",
    "    \"Given the size and the letter, return a H x W x C ndarray.\"\n",
    "    s = size_to_idx[size]\n",
    "    l = glia.letter_classes[letter]\n",
    "    return size_letter_to_frame[s,l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 100, 200, 300, 400, 500, 600]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 21 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the size_letter_to_frame ndarray for first 3 classes\n",
    "\n",
    "fig, axes = plt.subplots(3,n_sizes, figsize=(10,6))\n",
    "for s, size in enumerate(sizes):\n",
    "    for l, letter in enumerate(glia.letter_classes[:3]):\n",
    "        ax = axes[l,s]\n",
    "        ax.imshow(size_letter_to_frame[s,l])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(f\"class: {l}, size: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may take a few minutes\n",
    "training_100ms = glia.bin_100ms(np.expand_dims(data[\"training_data\"],0))\n",
    "validation_100ms = glia.bin_100ms(np.expand_dims(data[\"validation_data\"],0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i, size in enumerate(sizes):\n",
    "    print(f'==== SVC for size {size} ====')\n",
    "    # note: no expand dims, hardcoded 1 ncondition\n",
    "    training_target = data[\"training_target\"][i]\n",
    "    validation_target = data[\"validation_target\"][i]\n",
    "    svr = svm.SVC()\n",
    "    parameters = {'C': [1, 10, 100, 1000],\n",
    "                  'gamma': [0.001, 0.0001]},\n",
    "    clf = GridSearchCV(svr, parameters, n_jobs=12)\n",
    "    report, confusion = glia.classifier_helper(clf,\n",
    "        (training_100ms[0,i], training_target),\n",
    "        (validation_100ms[0,i], validation_target))\n",
    "    print(report+'\\n')\n",
    "    print(confusion)\n",
    "    \n",
    "# near perfect precision & recall at size=300 declines to only ~55%\n",
    "# at size=100. Note that chance is 9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel value inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, cv2\n",
    "from torch import nn, optim, utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F\n",
    "import torchvision.utils\n",
    "from dataclasses import dataclass\n",
    "\n",
    "torch.manual_seed(20200525); # reproducible analysis\n",
    "np.random.seed(seed=20200525)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7eff2f176630>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# choose crop parameters\n",
    "biggest = max(sizes)\n",
    "w1 = int((W - biggest*1.5) / 2)\n",
    "w2 = W - w1\n",
    "h1 = max(0, int((H - biggest*1.5) / 2))\n",
    "h2 = H - h1\n",
    "plt.imshow(size_letter_to_frame[-1,3,h1:h2,w1:w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((190, 1090), (0, 800))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w1, w2), (h1, h2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: SKIPPING 3 SIZES\n",
      "(N, T, H, W, C), (N, 2)\n",
      "train shapes:  (990, 2560) (990, 2)\n",
      "validation shapes:  (55, 2560) (55, 2)\n",
      "test shapes:  (55, 2560) (55, 2)\n"
     ]
    }
   ],
   "source": [
    "all_data = np.concatenate([training_100ms, validation_100ms], axis=2)[0]\n",
    "all_targets = np.concatenate([data[\"training_target\"], data[\"validation_target\"]], axis=1)\n",
    "\n",
    "# reshape to remove size dimension\n",
    "nskip = 3\n",
    "n_examples = np.product([all_data.shape[0]-nskip, all_data.shape[1]])\n",
    "X = np.zeros([n_examples, *all_data.shape[2:]])\n",
    "# to avoid blowing up RAM, we store (size, letter)\n",
    "# and will use dataloader to get the frame during training\n",
    "Y = np.zeros([n_examples, 2], dtype=np.uint8)\n",
    "\n",
    "print(f\"warning: SKIPPING {nskip} SIZES\")\n",
    "for s,size in enumerate(sizes[nskip:], start=nskip):\n",
    "    for n,(x,l) in enumerate(zip(all_data[s], all_targets[s])):\n",
    "        i = n * (n_sizes-nskip) + s - nskip\n",
    "        X[i] = x # time x H x W x C\n",
    "        Y[i,0] = s # size index\n",
    "        Y[i,1] = l # letter index\n",
    "        \n",
    "# del all_data, all_targets\n",
    "\n",
    "# 90% train, 5% validation, 5% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=1)\n",
    "\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=1)\n",
    "\n",
    "# del X, Y\n",
    "\n",
    "print(\"(N, T, H, W, C), (N, 2)\")\n",
    "print(\"train shapes: \", X_train.shape, Y_train.shape)\n",
    "print(\"validation shapes: \", X_val.shape, Y_val.shape)\n",
    "print(\"test shapes: \", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 11, 50, 56)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class LetterData(Dataset):\n",
    "    X: np.ndarray\n",
    "    Y: np.ndarray\n",
    "    size_letter_to_frame: np.ndarray\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        size_idx = self.Y[i,0]\n",
    "        letter_idx = self.Y[i,1]\n",
    "        frame = self.size_letter_to_frame[size_idx, letter_idx]\n",
    "        return self.X[i], frame\n",
    "        \n",
    "def resize_4d(images, fx, fy, interpolation=cv2.INTER_LINEAR, out=\"ndarray\",\n",
    "        is_tiff=False):\n",
    "    im = cv2.resize((images[0,0]), None, fx=fx, fy=fy, interpolation=interpolation)\n",
    "    if out==\"ndarray\":\n",
    "        new = np.zeros([images.shape[0],images.shape[1], *im.shape],\n",
    "            dtype = np.float32)\n",
    "    elif out==\"memmap\":\n",
    "        new = np.memmap(\"glia_temp_memmap.mmap\", np.float32, \"w+\",\n",
    "            (images.shape[0],images.shape[1], im.shape[0],im.shape[1]))\n",
    "    for b, vol in enumerate(images):\n",
    "        for z, img in enumerate(vol):\n",
    "            new[b,z] = cv2.resize((img), None, fx=fx, fy=fy,\n",
    "                interpolation=interpolation)\n",
    "    return new\n",
    "\n",
    "# downsample to 0.5x\n",
    "small_size_letter_to_frame = resize_4d(size_letter_to_frame[:,:,h1:h2, w1:w2],1/16,1/16)[...,0]\n",
    "# binarize\n",
    "small_size_letter_to_frame = (small_size_letter_to_frame > 128).astype(np.float32)\n",
    "\n",
    "train_data = LetterData(X_train, Y_train, small_size_letter_to_frame)\n",
    "val_data = LetterData(X_val, Y_val, small_size_letter_to_frame)\n",
    "test_data = LetterData(X_test, Y_test, small_size_letter_to_frame)\n",
    "print(small_size_letter_to_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at smallest letter with downsampling \n",
    "# NOTE: WE SKIP THE SMALLEST\n",
    "plt.imshow(small_size_letter_to_frame[nskip,1])\n",
    "small_H, small_W = small_size_letter_to_frame[0,1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether or not to use cuda\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kingma & Welling (2014) style variational autoencoder\n",
    "\n",
    "# subclass PyTorch Module for reverse-mode autodifferentiation \n",
    "# for easy backpropogation of loss gradient\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_latent, n_hidden2, n_output,\n",
    "                 nonlinearity=F.sigmoid):\n",
    "        super(VAE, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.n_hidden = n_hidden\n",
    "        self.nonlinearity = F.sigmoid\n",
    "                \n",
    "        # Encoder layers\n",
    "        self.hidden_encoder = nn.Linear(n_input, n_hidden)\n",
    "        # mean encoding layer \n",
    "        self.mean_encoder = nn.Linear(n_hidden, n_latent)\n",
    "        # log variance encoding layer \n",
    "        self.logvar_encoder = nn.Linear(n_hidden, n_latent)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.hidden_decoder = nn.Linear(n_latent, n_hidden2)\n",
    "        self.reconstruction_decoder = nn.Linear(n_hidden2, n_output)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.nonlinearity(self.hidden_encoder(x))\n",
    "        return self.mean_encoder(h1), self.logvar_encoder(h1)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        \"\"\"Reparameterize out stochastic node so the gradient can propogate \n",
    "           deterministically.\"\"\"\n",
    "\n",
    "        if self.training:\n",
    "            standard_deviation = torch.exp(0.5*logvar)\n",
    "            # sample from unit gaussian with same shape as standard_deviation\n",
    "            epsilon = torch.randn_like(standard_deviation)\n",
    "            return epsilon * standard_deviation + mean\n",
    "        else:\n",
    "            return mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.nonlinearity(self.hidden_decoder(z))\n",
    "        # bound output to (0,1)\n",
    "        return F.sigmoid(self.reconstruction_decoder(h3))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"A special method in PyTorch modules that is called by __call__\"\n",
    "        mean, logvar = self.encode(x)\n",
    "        # sample an embedding, z\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        # return the (sampled) reconstruction, mean, and log variance\n",
    "        return self.decode(z), mean, logvar\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(recon_y, y, mu, logvar, beta=1):\n",
    "    \"Reconstruction + KL divergence losses summed over all elements and batch.\"\n",
    "    BCE = F.binary_cross_entropy(recon_y, y, size_average=False)\n",
    "\n",
    "    # we want KLD = - 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # where sigma is standard deviation and mu is mean\n",
    "    # (see Appendix B of https://arxiv.org/abs/1312.6114)\n",
    "    \n",
    "    # see https://openreview.net/forum?id=Sy2fzU9gl for info on choosing Beta\n",
    "    \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "\n",
    "    return BCE + beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, train_loader, beta=1., log_interval=10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        bz = data[0].shape[0]\n",
    "        # flatten batch x height x width x channel into batch x nFeatures\n",
    "        X = data[0].reshape(bz, -1).to(device)\n",
    "        Y = data[1].reshape(bz, -1).to(device) # flatten\n",
    "        optimizer.zero_grad()\n",
    "        Y_pred, mu, logvar = model(X)\n",
    "        loss = loss_function(Y_pred, Y, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch % log_interval == 0:\n",
    "        print('Epoch: {} Average loss: {:.4f}'.format(\n",
    "              epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch, model, test_loader, H, W, beta=1., log_interval=10,\n",
    "         save_image=True, folder=\"results\", name=\"Test\"):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            bz = data[0].shape[0]\n",
    "            \n",
    "            # flatten batch x height x width x channel into batch x nFeatures\n",
    "            X = data[0].reshape(bz, -1).to(device)\n",
    "            Y = data[1].reshape(bz, -1).to(device) # flatten\n",
    "\n",
    "            Y_pred, mu, logvar = model(X)\n",
    "            test_loss += loss_function(Y_pred, Y, mu, logvar).item()\n",
    "            if (i == 0) and save_image:\n",
    "                n = min(X.size(0), 15)\n",
    "                comparison = torch.cat([Y[:n].view(-1,1,H,W),\n",
    "                                   Y_pred[:n].view(-1, 1, H, W)])\n",
    "                torchvision.utils.save_image(comparison.cpu(),\n",
    "                         folder+f'/{name}_reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    if epoch % log_interval == 0:\n",
    "        print(f'====> {name} ' + 'set loss: {:.4f}'.format(test_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5364000\n",
      "Epoch: 10 Average loss: 1007.3247\n",
      "====> Validation set loss: 998.3933\n",
      "Epoch: 20 Average loss: 876.1274\n",
      "====> Validation set loss: 880.9705\n",
      "Epoch: 30 Average loss: 810.4409\n",
      "====> Validation set loss: 819.8396\n",
      "Epoch: 40 Average loss: 709.4315\n",
      "====> Validation set loss: 744.5324\n",
      "Epoch: 50 Average loss: 646.7073\n",
      "====> Validation set loss: 716.6578\n",
      "Epoch: 60 Average loss: 607.9758\n",
      "====> Validation set loss: 703.3165\n",
      "Epoch: 70 Average loss: 577.7620\n",
      "====> Validation set loss: 693.8460\n",
      "Epoch: 80 Average loss: 554.1780\n",
      "====> Validation set loss: 690.3384\n",
      "Epoch: 90 Average loss: 535.2322\n",
      "====> Validation set loss: 692.7732\n",
      "Epoch: 100 Average loss: 519.7425\n",
      "====> Validation set loss: 697.2898\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(LetterData(X_train.astype(np.float32),\n",
    "        Y_train, small_size_letter_to_frame),\n",
    "    batch_size=64, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(LetterData(X_val.astype(np.float32),\n",
    "        Y_val, small_size_letter_to_frame),\n",
    "    batch_size=64, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(LetterData(X_test.astype(np.float32),\n",
    "        Y_test, small_size_letter_to_frame),\n",
    "    batch_size=64, shuffle=True, pin_memory=True)\n",
    "\n",
    "nfeatures = 28**2\n",
    "# we use a latent space of dimension 2 as to get an easy-to-visualize manifold\n",
    "# (see mnist/sample_*.png while running next cell)\n",
    "n_input = np.prod(X_train.shape[1:])\n",
    "n_hidden = 1000\n",
    "n_latent = 2\n",
    "n_hidden2 = 1000\n",
    "beta = 10\n",
    "lr = 3e-5\n",
    "nonlinearity = F.celu\n",
    "n_output = np.product(small_size_letter_to_frame.shape[2:])\n",
    "est_params = n_input*n_hidden + n_hidden*n_latent + n_latent*n_hidden2 + n_hidden2*n_output\n",
    "print(est_params)\n",
    "\n",
    "model = VAE(n_input, n_hidden, n_latent, n_hidden2, n_output,\n",
    "            nonlinearity=nonlinearity).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "!mkdir -p letters_viz && rm letters_viz/*\n",
    "\n",
    "# this will take two minutes to run or so\n",
    "# As it does, check out the letters_viz folder!\n",
    "# each epoch, reconstruction examples are saved (original on top) \n",
    "# select files on right, then refresh, and double click image\n",
    "# click bottom right corner of image to resize\n",
    "\n",
    "nepochs = 100\n",
    "\n",
    "# make grid of z1 x z2 where z1,z2 \\elem (-3.5,-2.5, ..., 3.5)\n",
    "nrow = 25\n",
    "latents = torch.zeros(nrow,nrow,n_latent)\n",
    "z1_tick = np.linspace(-3.5,3.5,nrow)\n",
    "z2_tick = np.linspace(-3.5,3.5,nrow)\n",
    "for i, z1 in enumerate(z1_tick):\n",
    "    for j, z2 in enumerate(z2_tick):\n",
    "        latents[i,j,[0,1]] = torch.tensor([z1,z2])\n",
    "latents = latents.to(device)\n",
    "\n",
    "for epoch in range(1, nepochs + 1):\n",
    "    if epoch % 10 == 0:\n",
    "        save_image = True\n",
    "    else:\n",
    "        save_image = False\n",
    "    train(epoch, model, optimizer, train_loader, beta=beta)\n",
    "    test(epoch, model, val_loader, small_H, small_W, beta=beta,\n",
    "         save_image=save_image, folder='letters_viz', name=\"Validation\")\n",
    "    if save_image:\n",
    "        with torch.no_grad():\n",
    "            latent_space = model.decode(latents.view(-1,n_latent)).cpu()\n",
    "            torchvision.utils.save_image(latent_space.view(-1, 1, small_H, small_W),\n",
    "                       'letters_viz/sample_' + str(epoch) + '.png',nrow=nrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 665.7424\n"
     ]
    }
   ],
   "source": [
    "test(epoch, model, test_loader, small_H, small_W, beta=beta,\n",
    "         save_image=save_image, folder='letters_viz', name=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
